from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def evaluate_retrieval(query_embedding, retrieved_embeddings):
    """
    Evaluates retrieval accuracy using cosine similarity.
    
    Args:
        query_embedding (np.array): The embedding of the query.
        retrieved_embeddings (List[np.array]): Embeddings of retrieved documents.
    
    Returns:
        List[float]: Cosine similarity scores for each retrieved document.
    """
    similarities = cosine_similarity([query_embedding], retrieved_embeddings)
    return similarities[0].tolist()

def evaluate_llm_response(user_query, llm_response, ground_truth):
    """
    Evaluates LLM response relevance using semantic similarity (or BLEU scores).
    
    Args:
        user_query (str): The user's input query.
        llm_response (str): The response generated by the LLM.
        ground_truth (str): The expected correct response.
    
    Returns:
        float: Similarity score (or BLEU score if implemented).
    """
    # Simple word overlap ratio for now (can replace with BLEU or other metrics)
    user_words = set(user_query.lower().split())
    response_words = set(llm_response.lower().split())
    ground_truth_words = set(ground_truth.lower().split())
    
    intersection = response_words.intersection(ground_truth_words)
    return len(intersection) / len(ground_truth_words) if ground_truth_words else 0.0

if __name__ == "__main__":
    # Example usage for retrieval evaluation
    query_emb = np.array([0.1, 0.2, 0.3])
    retrieved_embs = [np.array([0.1, 0.2, 0.3]), np.array([0.4, 0.5, 0.6])]
    print("Retrieval Similarity Scores:", evaluate_retrieval(query_emb, retrieved_embs))
    
    # Example usage for LLM response evaluation
    query = "What are good post-workout meals?"
    response = "High-protein meals like chicken and eggs are great post-workout."
    truth = "Chicken and eggs are high-protein meals suitable after workouts."
    print("LLM Response Score:", evaluate_llm_response(query, response, truth))
